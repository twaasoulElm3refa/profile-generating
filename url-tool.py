import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from fastapi import Query
from fastapi import FastAPI
from database import fetch_profile_data ,insert_generated_profile
from pydantic import BaseModel
from typing import Optional
from fastapi.responses import FileResponse
import os
from dotenv import load_dotenv
import json
from openai import OpenAI

# ุชุญููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
load_dotenv()

app = FastAPI()
api_key=os.getenv("OPENAI_API_KEY")

def extract_info_from_url_and_subpages(base_url, max_pages=5):
    visited = set()
    to_visit = [base_url]
    all_texts = []

    while to_visit and len(visited) < max_pages:
        url = to_visit.pop(0)
        if url in visited:
            continue
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            html = response.text
            soup = BeautifulSoup(html, "html.parser")

            visited.add(url)

            # ุฌูุน ุงููุตูุต
            page_text = []

            # ุงูุนููุงู
            title = soup.title.string.strip() if soup.title and soup.title.string else ""
            if title:
                page_text.append(f"Title: {title}")

            # meta description
            desc_tag = soup.find("meta", attrs={"name": "description"})
            if desc_tag and desc_tag.get("content"):
                page_text.append(f"Description: {desc_tag['content'].strip()}")

            # ุฃูู ููุฑุชูู ุทูููุชูู
            paragraphs = soup.find_all("p")
            count = 0
            for p in paragraphs:
                text = p.get_text(strip=True)
                if len(text) > 50:
                    page_text.append(text)
                    count += 1
                if count >= 2:
                    break

            all_texts.append("\n".join(page_text))

            # ุงุณุชุฎุฑุงุฌ ุฑูุงุจุท ุฏุงุฎููุฉ ุฌุฏูุฏุฉ ูุฒูุงุฑุชูุง ูุงุญููุง
            for link in soup.find_all("a", href=True):
                href = link["href"]
                joined_url = urljoin(base_url, href)
                parsed_base = urlparse(base_url)
                parsed_joined = urlparse(joined_url)
                # ูุชุฃูุฏ ุฃู ุงูุฑุงุจุท ุฏุงุฎูู (ููุณ ุงูุฏูููู)
                if parsed_base.netloc == parsed_joined.netloc:
                    if joined_url not in visited and joined_url not in to_visit:
                        to_visit.append(joined_url)

        except Exception as e:
            print(f"โ Error visiting {url}: {e}")
            continue

    # ุฏูุฌ ูู ุงููุตูุต ุงููุณุชุฎุฑุฌุฉ
    combined_text = "\n\n---\n\n".join(all_texts)
    return combined_text


def load_examples_from_json(json_path="example_profiles.json"):
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)


def generate_profile_model(data, examples):

    client = OpenAI(api_key=api_key)
    examples_text = "\n\n".join(examples[:2])  # ูุฑุณู ุฃูู ูุซุงููู ููุท ูุชูููู ุงูุทูู
    prompt=f"""
ุฃูุช ุฎุจูุฑ ูุญุชุฑู ูู ุฅุนุฏุงุฏ ุงููููุงุช ุงูุชุนุฑูููุฉ ููุดุฑูุงุช (Company Profiles)ุ ูุชุนูู ููุณุชุดุงุฑ ุงุณุชุฑุงุชูุฌู ูุชุทููุฑ ุงููููุฉ ุงููุคุณุณูุฉ ูุตูุงุบุฉ ุงููุญุชูู ุงูุชุณูููู ุงูุงุญุชุฑุงูู.

ุณุชุชููู:
- ุฃูุซูุฉ ุญููููุฉ ููููุงุช ุชุนุฑูููุฉ ูุงุฌุญุฉ ูุนุฏุฉ ุดุฑูุงุช:
{examples_text}

ููุนูููุงุช ุฃุณุงุณูุฉ ุชู ุงุณุชุฎุฑุงุฌูุง ูุจุงุดุฑุฉู ูู ูููุน ุงูุดุฑูุฉ (URL):
{data}

---

๐ ุงููุทููุจ ููู:
1๏ธโฃ ุชุญููู ุงูุฃูุซูุฉ ุงููุงุฑุฏุฉ ูุงุณุชุฎูุงุต ุฃุณููุจ ุงุญุชุฑุงูู ูุชูุงูู ูู ูุชุงุจุฉ ุงููููุงุช ุงูุชุนุฑูููุฉ.
2๏ธโฃ ุงูุงุณุชูุงุฏุฉ ูู ุงูุจูุงูุงุช ุงููุณุชุฎุฑุฌุฉ ูู ุงููููุน (url) ููุง ูู ุชูุงููุงุ ูุฅู ูู ุชูู ุงููุนูููุงุช ููุชููุฉุ ูู ุจุฅููุงููุง ูุงุจุชูุงุฑ ูุญุชูู ูููู ุจุฃุณููุจ ูุชูุงุณู.
3๏ธโฃ ูุชุงุจุฉ ููู ุชุนุฑููู ูุชูุงูู ูุดูู:
   - ูู ูุญู
   - ุงูุฑุคูุฉ (ูู ููุฑุฉ ูููุตูุฉ)
   - ุงูุฑุณุงูุฉ (ูู ููุฑุฉ ูููุตูุฉ)
   - ูุง ุงูุฐู ูููุฏูู
   - ููุงุฐุง ูุญู
   - ุฃุนูุงููุง
   - ุฎุฏูุงุชูุง (ููุตูุฉ ุจููุงุท)
   - ุฃุณููุจูุง
   - ูุนูููุงุช ุงูุชูุงุตู

---

โ ุชุนูููุงุช ุฃุณุงุณูุฉ:
- ุงุณุชุฎุฏู ุฃุณููุจ ุนุตุฑู ูุฌุฐุงุจ ููุงุฒู ุจูู ุงููุต ุงูุชุณูููู ูุงููุนูููุงุชู.
- ูุง ุชุนุชูุฏ ุนูู ูููู ุฌุงูุฒ ุญุฑูููุงุ ุงุจุชูุฑ ุชุฑุชูุจูุง ุชุฏุฑูุฌููุง ููุงุณุจ ูุฌุงู ุงูุดุฑูุฉ.
- ุงุฌุนู ุงููุต ุบูููุง ุจุงูุชูุงุตูู ููุนูุณ ุงููููุฉ ุงูุชูุงูุณูุฉ ุงููุณุชุฎูุตุฉ ูู ุงูุฃูุซูุฉ.
- ุงุณุชุฎุฏู ูุบุฉ ูุคุณุณูุฉ ุณูุณุฉ ููุชูุงุณูุฉ ุจุตุฑููุง ููุถูููููุง.
- ุงูุชุฑุถ ุฃู ุงูููู ุณููุณุชุฎุฏู ููุทุจุงุนุฉ ุงููุงุฎุฑุฉ ูุงูุนุฑูุถ ุงูุฅููุชุฑูููุฉ ูุงูุชูุฏูููุฉ.
- ุงูููู ูุฌุจ ุฃู ููุฌุณูุฏ ูููุฉ ุงูุดุฑูุฉ ููููุน ุงูุฌูุงุช ุงูุงุณุชุซูุงุฑูุฉ ูุงูุนููุงุก ุงููุณุชูุฏููู.

---

โจ ุงููุฏู:
ุฅูุดุงุก ููู ุชุนุฑููู ููู ูุนุจุฑ ุนู ุฑูุญ ุงูุดุฑูุฉ ุจุฃุณููุจ ูุณุชูุญู ููุชุนูููู ูู ุงูุฃูุซูุฉ ุงููุงุฑุฏุฉุ ูุน ููุก ุฃู ููุต ูู ุจูุงูุงุช ุงููููุน ุชููุงุฆููุง ุจุฃุณููุจ ุงุญุชุฑุงูู.
"""


    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content



@app.get("/profile-generating-tool-from-url/{user_id}/")
def profile_from_url(user_id ,url: str = Query(..., description="Company website URL") ):
    # ุงุณุชุฎุฑุฌ ุงูุจูุงูุงุช ูู ุงูุฑุงุจุท
    extracted_data = extract_info_from_url_and_subpages(url)
    
    # ููุฑุฃ ุงูุฃูุซูุฉ
    loaded_examples = load_examples_from_json()

    # ุชูููุฏ ุงูุจุฑููุงูู
    generated_profile = generate_profile_model(extracted_data, loaded_examples)

    input_type='Using URL'
    # ูููู ุชุญูุธู ูู db ุฅุฐุง ุนูุฏู ุฌุฏูู ุฎุงุต ุจุงูุฑุงุจุท
    save_data= insert_generated_profile(user_id,None,generated_profile,input_type)
    # ุฃู ุชุฑุณูู ูุจุงุดุฑุฉ ูููุงุฌูุฉ
    return {"profile": generated_profile}

